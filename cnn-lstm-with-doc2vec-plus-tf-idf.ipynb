{"cells":[{"metadata":{"_cell_guid":"c89c6d78-cd49-43b4-99b7-262151427d57","_uuid":"bc244d28a1532f3842dd1739cec5adff4e4b6c79","collapsed":true,"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom keras.preprocessing import sequence\nfrom keras.layers import TimeDistributed, GlobalAveragePooling1D, GlobalAveragePooling2D, BatchNormalization\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D, Conv2D, MaxPooling2D, AveragePooling1D\n#from keras.layers.embeddings import Embedding\nfrom keras.layers import Dropout, Flatten, Bidirectional, Dense, Activation, TimeDistributed\nfrom keras.models import Model, Sequential\nfrom keras.utils import np_utils\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom string import ascii_lowercase\nfrom collections import Counter\nfrom gensim.models import Word2Vec\nfrom gensim.models import Doc2Vec\nfrom gensim.models import doc2vec\nfrom gensim.models import KeyedVectors\nimport itertools, nltk, snowballstemmer, re\n\nLabeledSentence = doc2vec.LabeledSentence","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"66eaff70-1bb2-436f-ac31-b85281a2876d","_uuid":"4402fef233db64bf46f51e3bdd0aadc111b11753","collapsed":true,"trusted":true},"cell_type":"code","source":"class LabeledLineSentence(object):\n    def __init__(self, sources):\n        self.sources = sources\n        \n        flipped = {}\n        \n        # make sure that keys are unique\n        for key, value in sources.items():\n            if value not in flipped:\n                flipped[value] = [key]\n            else:\n                raise Exception('Non-unique prefix encountered')\n    \n    def __iter__(self):\n        for source, prefix in self.sources.items():\n            with utils.smart_open(source) as fin:\n                for item_no, line in enumerate(fin):\n                    yield LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no])\n    \n    def to_array(self):\n        self.sentences = []\n        for source, prefix in self.sources.items():\n            with utils.smart_open(source) as fin:\n                for item_no, line in enumerate(fin):\n                    self.sentences.append(LabeledSentence(utils.to_unicode(line).split(), [prefix + '_%s' % item_no]))\n        return self.sentences\n    \n    def sentences_perm(self):\n        shuffled = list(self.sentences)\n        random.shuffle(shuffled)\n        return shuffled","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"eac514b4-5efb-4291-81a3-6730a0cdac61","_uuid":"57bed5b9fd3ad963809205d71d0883a9e38bb9f9","collapsed":true,"trusted":true},"cell_type":"code","source":"#data = pd.read_csv('deceptive-opinion-spam-corpus.zip', compression='zip', header=0, sep=',', quotechar='\"')\ndata = pd.read_csv(\"../input/deceptive-opinion.csv\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5aedff72-437f-4530-9bfe-56165ddd9690","_uuid":"0d7d60b7569b847685579a73ded95641fcfe63d1","collapsed":true,"trusted":true},"cell_type":"code","source":"data['polarity'] = np.where(data['polarity']=='positive', 1, 0)\ndata['deceptive'] = np.where(data['deceptive']=='truthful', 1, 0)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1a30e639-f406-4320-acd4-b190cb0a1f06","_uuid":"fb20c77b00518d320c96687071080ac60b3ae8e7","collapsed":true,"trusted":true},"cell_type":"code","source":"def create_class(c):\n    if c['polarity'] == 1 and c['deceptive'] == 1:\n        return [1,1]\n    elif c['polarity'] == 1 and c['deceptive'] == 0:\n        return [1,0]\n    elif c['polarity'] == 0 and c['deceptive'] == 1:\n        return [0,1]\n    else:\n        return [0,0]\n    \ndef specific_class(c):\n    if c['polarity'] == 1 and c['deceptive'] == 1:\n        return \"TRUE_POSITIVE\"\n    elif c['polarity'] == 1 and c['deceptive'] == 0:\n        return \"FALSE_POSITIVE\"\n    elif c['polarity'] == 0 and c['deceptive'] == 1:\n        return \"TRUE_NEGATIVE\"\n    else:\n        return \"FALSE_NEGATIVE\"\n\ndata['final_class'] = data.apply(create_class, axis=1)\ndata['given_class'] = data.apply(specific_class, axis=1)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"4eaeb14e-a7f3-455e-b43c-64b26bdccd7b","_uuid":"6151435f72f8802262e5b748a4039bbc4074943f","collapsed":true,"trusted":true},"cell_type":"code","source":"Y = data['final_class']\n# encode class values as integers\nencoder = LabelEncoder()\nencoder.fit(Y)\nencoded_Y = encoder.transform(Y)\n# convert integers to dummy variables (i.e. one hot encoded)\ndummy_y = np_utils.to_categorical(encoded_Y)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"358caced-4efe-454d-b9e6-fb1db61c3cd5","_uuid":"5a2723885623b4e5c62d5cbf04b639450ebcd486","collapsed":true,"trusted":true},"cell_type":"code","source":"textData = pd.DataFrame(list(data['text'])) # each row is one document; the raw text of the document should be in the 'text_data' column","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"727195f6-a01b-478c-9e6e-f63f9e0bcb51","_uuid":"b4b122f69da68f42742cf97da439dc067b1a5a52","collapsed":true,"trusted":true},"cell_type":"code","source":"# initialize stemmer\nstemmer = snowballstemmer.EnglishStemmer()\n\n# grab stopword list, extend it a bit, and then turn it into a set for later\nstop = stopwords.words('english')\nstop.extend(['may','also','zero','one','two','three','four','five','six','seven','eight','nine','ten','across','among','beside','however','yet','within']+list(ascii_lowercase))\nstoplist = stemmer.stemWords(stop)\nstoplist = set(stoplist)\nstop = set(sorted(stop + list(stoplist))) ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3bf58a8f-b3aa-41d0-af3b-4eca01d7f7bb","_uuid":"8a53f638d29e5bf6c6859dff180ddd45611c6f4a","collapsed":true,"trusted":true},"cell_type":"code","source":"# remove characters and stoplist words, then generate dictionary of unique words\ntextData[0].replace('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ',inplace=True,regex=True)\nwordlist = filter(None, \" \".join(list(set(list(itertools.chain(*textData[0].str.split(' ')))))).split(\" \"))\ndata['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in stop, line))) for line in textData[0].str.lower().str.split(' ')]","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0a714384-6691-4178-94f8-6bc7e7771908","_uuid":"7c57c1e8ed3f743e63850290bf5ed6030dcb95e3","collapsed":true,"trusted":true},"cell_type":"code","source":"# remove all words that don't occur at least 5 times and then stem the resulting docs\nminimum_count = 1\nstr_frequencies = pd.DataFrame(list(Counter(filter(None,list(itertools.chain(*data['stemmed_text_data'].str.split(' '))))).items()),columns=['word','count'])\nlow_frequency_words = set(str_frequencies[str_frequencies['count'] < minimum_count]['word'])\ndata['stemmed_text_data'] = [' '.join(filter(None,filter(lambda word: word not in low_frequency_words, line))) for line in data['stemmed_text_data'].str.split(' ')]\ndata['stemmed_text_data'] = [\" \".join(stemmer.stemWords(re.sub('[!\"#%\\'()*+,-./:;<=>?@\\[\\]^_`{|}~1234567890’”“′‘\\\\\\]',' ', next_text).split(' '))) for next_text in data['stemmed_text_data']]    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ad04c9ec-5721-4a55-bcec-afd7f4e150db","_uuid":"3067446c5a55f58f67fcc4d1eb9cb1f0eea8a339","collapsed":true,"trusted":true},"cell_type":"code","source":"lmtzr = WordNetLemmatizer()\nw = re.compile(\"\\w+\",re.I)\n\ndef label_sentences(df, input_point):\n    labeled_sentences = []\n    list_sen = []\n    for index, datapoint in df.iterrows():\n        tokenized_words = re.findall(w,datapoint[input_point].lower())\n        labeled_sentences.append(LabeledSentence(words=tokenized_words, tags=['SENT_%s' %index]))\n        list_sen.append(tokenized_words)\n    return labeled_sentences, list_sen\n\ndef train_doc2vec_model(labeled_sentences):\n    model = Doc2Vec(min_count=1, window=9, size=512, sample=1e-4, negative=5, workers=7)\n    model.build_vocab(labeled_sentences)\n    pretrained_weights = model.wv.syn0\n    vocab_size, embedding_size = pretrained_weights.shape\n    model.train(labeled_sentences, total_examples=vocab_size, epochs=400)\n    \n    return model","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1cab13d3-d608-4e49-89fc-5076835f4ab2","_uuid":"2bf3e3c0bef170d0601e098b159ea758a3aeb461","collapsed":true,"trusted":true},"cell_type":"code","source":"textData = data['stemmed_text_data'].to_frame().reset_index()\nsen, corpus = label_sentences(textData, 'stemmed_text_data')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a84fe753-6f2c-44d9-990a-af41a34a282b","_uuid":"fdf41cbef8aed8c77625debd753d88b37ed108ca","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"doc2vec_model = train_doc2vec_model(sen)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bcbdbe26-8a3f-4350-8633-3d9aaf0c172e","_uuid":"4de396cec1aeb9a63648d1aa6ac93d3914c05b78","collapsed":true,"trusted":true},"cell_type":"code","source":"doc2vec_model.save(\"doc2vec_model_opinion_corpus.d2v\")","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3fe3f2b0-b681-421c-9adc-682796f97387","_uuid":"161eecd1a6baac4e39bfa9f5b7620ece1165ecc7","collapsed":true,"trusted":true},"cell_type":"code","source":"doc2vec_model = Doc2Vec.load(\"doc2vec_model_opinion_corpus.d2v\") ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"e4bc5f09-3bbf-4ddf-97e1-e497c210d6ae","_uuid":"fb98f66f270a63bfd0d82bb1cfcacd18dae40873","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.decomposition import TruncatedSVD\n\ntfidf1 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,1))\nresult_train1 = tfidf1.fit_transform(corpus)\n\ntfidf2 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,2))\nresult_train2 = tfidf2.fit_transform(corpus)\n\ntfidf3 = TfidfVectorizer(tokenizer=lambda i:i, lowercase=False, ngram_range=(1,3))\nresult_train3 = tfidf3.fit_transform(corpus)\n\nsvd = TruncatedSVD(n_components=512, n_iter=40, random_state=34)\ntfidf_data1 = svd.fit_transform(result_train1)\ntfidf_data2 = svd.fit_transform(result_train2)\ntfidf_data3 = svd.fit_transform(result_train3)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"ef1dbd5a-e69e-4b56-9ff3-c74ce62fdfdd","_uuid":"bfd26d39cd104b2bf443fc52992437751fa852b7","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.feature_extraction.text import CountVectorizer\nimport spacy\n\nnlp = spacy.load('en')\ntemp_textData = pd.DataFrame(list(data['text']))\n\noverall_pos_tags_tokens = []\noverall_pos = []\noverall_tokens = []\noverall_dep = []\n\nfor i in range(1600):\n    doc = nlp(temp_textData[0][i])\n    given_pos_tags_tokens = []\n    given_pos = []\n    given_tokens = []\n    given_dep = []\n    for token in doc:\n        output = \"%s_%s\" % (token.pos_, token.tag_)\n        given_pos_tags_tokens.append(output)\n        given_pos.append(token.pos_)\n        given_tokens.append(token.tag_)\n        given_dep.append(token.dep_)\n        \n    overall_pos_tags_tokens.append(given_pos_tags_tokens)\n    overall_pos.append(given_pos)\n    overall_tokens.append(given_tokens)\n    overall_dep.append(given_dep)\n","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dbcdbe6d-0c7f-44dc-a9d6-0f240d339885","_uuid":"1c19ee8bd8fb5862cc5d25225b0684dd755ee8b2","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn.preprocessing import MinMaxScaler\n\ncount = CountVectorizer(tokenizer=lambda i:i, lowercase=False)\npos_tags_data = count.fit_transform(overall_pos_tags_tokens).todense()\npos_data = count.fit_transform(overall_pos).todense()\ntokens_data = count.fit_transform(overall_tokens).todense()\ndep_data = count.fit_transform(overall_dep).todense()\nmin_max_scaler = MinMaxScaler()\nnormalized_pos_tags_data = min_max_scaler.fit_transform(pos_tags_data)\nnormalized_pos_data = min_max_scaler.fit_transform(pos_data)\nnormalized_tokens_data = min_max_scaler.fit_transform(tokens_data)\nnormalized_dep_data = min_max_scaler.fit_transform(dep_data)\n\nfinal_pos_tags_data = np.zeros(shape=(1600, 512)).astype(np.float32)\nfinal_pos_data = np.zeros(shape=(1600, 512)).astype(np.float32)\nfinal_tokens_data = np.zeros(shape=(1600, 512)).astype(np.float32)\nfinal_dep_data = np.zeros(shape=(1600, 512)).astype(np.float32)\nfinal_pos_tags_data[:normalized_pos_tags_data.shape[0],:normalized_pos_tags_data.shape[1]] = normalized_pos_tags_data\nfinal_pos_data[:normalized_pos_data.shape[0],:normalized_pos_data.shape[1]] = normalized_pos_data\nfinal_tokens_data[:normalized_tokens_data.shape[0],:normalized_tokens_data.shape[1]] = normalized_tokens_data\nfinal_dep_data[:normalized_dep_data.shape[0],:normalized_dep_data.shape[1]] = normalized_dep_data","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"57e0d758-a2b5-4a24-a60e-cfa2df06572f","_uuid":"e6474b59e587fbafbd2ea8801a50ef6d787a5868","collapsed":true,"trusted":true},"cell_type":"code","source":"maxlength = []\nfor i in range(0,len(sen)):\n    maxlength.append(len(sen[i][0]))\n    \nprint(max(maxlength))   ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"dda366e0-43b7-4b35-8f31-c41d572375d4","_uuid":"f53332f2ce02fee41617d0205c524ebe26c4a90d","collapsed":true,"trusted":true},"cell_type":"code","source":"def vectorize_comments(df,d2v_model):\n    y = []\n    comments = []\n    for i in range(0,df.shape[0]):\n        label = 'SENT_%s' %i\n        comments.append(d2v_model.docvecs[label])\n    df['vectorized_comments'] = comments\n    \n    return df\n\ntextData = vectorize_comments(textData,doc2vec_model)\nprint (textData.head(2))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3fafb37f-8eee-4de3-80a0-0587f7075bc7","_uuid":"b978f4c41b6adc60490f0c6e048b3195249b9629","collapsed":true,"trusted":true},"cell_type":"code","source":"# load the whole embedding into memory\n#embeddings_index = dict()\n#f = open('glove/glove.6B.300d.txt')\n#for line in f:\n#    values = line.split()\n#    word = values[0]\n#    coefs = np.asarray(values[1:], dtype='float32')\n#    embeddings_index[word] = coefs\n#f.close()\n#print('Loaded %s word vectors.' % len(embeddings_index))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"08eadfe1-897d-4e46-bc46-95a5e452a618","_uuid":"b99088a77d8d46e52ba16259ffba4b655265f3cd","collapsed":true,"trusted":true},"cell_type":"code","source":"#from nltk.corpus import stopwords\n\n#glove_data = np.zeros(shape=(1600, 800, 512)).astype(np.float32)\n#temp_textData = data['text'].to_frame().reset_index()\n#sen2, corpus2 = label_sentences(temp_textData, 'text')\n#stop_words = set(stopwords.words('english'))\n#test_word = np.zeros(512).astype(np.float32)\n#final_matrix = np.zeros(512).astype(np.float32)\n#final_sizes = []\n\n#count = True\n\n#for i in range(1600):\n#    for j in sen2[i][0]:\n#        if j in embeddings_index and j not in stop_words:\n#            test_word[:300] = embeddings_index[j]\n#            if count == True:\n#                final_matrix = test_word\n#                count = False\n#            else:\n#                final_matrix = np.vstack((final_matrix, test_word))\n            \n#    final_sizes.append(final_matrix.shape[0])\n#    final_matrix = np.zeros(512).astype(np.float32)\n#    glove_data[i,:final_matrix.shape[0],:] = final_matrix\n#    count = True\n    \n#print(max(final_sizes))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"b2a454a7-34dd-4eb5-93ac-ac891eb7e9fe","_uuid":"4ebed90a6e28ab4de06c3037761106b6f6a0f4e5","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"from sklearn import cross_validation\nfrom sklearn.grid_search import GridSearchCV\nX_train, X_test, y_train, y_test = cross_validation.train_test_split(textData[\"vectorized_comments\"].T.tolist(), \n                                                                     dummy_y, \n                                                                     test_size=0.1, \n                                                                     random_state=56)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"19c5322f-c579-4b9b-91fb-ebca06c6715f","_uuid":"2c4e424e1c6f1a3a6abb7beb8eec6f33d3bc2622","collapsed":true,"trusted":true},"cell_type":"code","source":"X = np.array(textData[\"vectorized_comments\"].T.tolist()).reshape((1,1600,512))\ny = np.array(dummy_y).reshape((1600,4))\nX_train2 = np.array(X_train).reshape((1,1440,512))\ny_train2 = np.array(y_train).reshape((1,1440,4))\nX_test2 = np.array(X_test).reshape((1,160,512))\ny_test2 = np.array(y_test).reshape((1,160,4))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"00a664f3-390b-4f40-9580-8ca6c3021678","_uuid":"87b1e9251955272eee9475662179a60dc030a399","collapsed":true,"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nXtemp = textData[\"vectorized_comments\"].T.tolist()\nytemp = data['given_class']\ntraining_indices = []\ntesting_indices = []\n\nskf = StratifiedKFold(n_splits=10)\nskf.get_n_splits(Xtemp, ytemp)\n\nfor train_index, test_index in skf.split(Xtemp, ytemp):\n    training_indices.append(train_index)\n    testing_indices.append(test_index)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"0fac1165-69ad-4576-9729-8857d625cef1","_uuid":"45bc95366bbb2e542be56aaf5f73582fd65062d4","collapsed":true,"trusted":true},"cell_type":"code","source":"def extractTrainingAndTestingData(givenIndex):\n    X_train3 = np.zeros(shape=(1440, max(maxlength)+10, 512)).astype(np.float32)\n    Y_train3 = np.zeros(shape=(1440, 4)).astype(np.float32)\n    X_test3 = np.zeros(shape=(160, max(maxlength)+10, 512)).astype(np.float32)\n    Y_test3 = np.zeros(shape=(160, 4)).astype(np.float32)\n\n    empty_word = np.zeros(512).astype(np.float32)\n\n    count_i = 0\n    for i in training_indices[givenIndex]:\n        len1 = len(sen[i][0])\n        average_vector1 = np.zeros(512).astype(np.float32)\n        average_vector2 = np.zeros(512).astype(np.float32)\n        average_vector3 = np.zeros(512).astype(np.float32)\n        for j in range(max(maxlength)+10):\n            if j < len1:\n                X_train3[count_i,j,:] = doc2vec_model[sen[i][0][j]]\n                average_vector1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n            #elif j >= len1 and j < len1 + 379:\n            #    X_train3[count_i,j,:] = glove_data[i, j-len1, :]\n            elif j == len1:\n                X_train3[count_i,j,:] = tfidf_data1[i]\n            elif j == len1 + 1:\n                X_train3[count_i,j,:] = tfidf_data2[i]\n            elif j == len1+2:\n                X_train3[count_i,j,:] = tfidf_data3[i]\n            elif j == len1+3:\n                X_train3[count_i,j,:] = average_vector1\n            elif j == len1+4:\n                X_train3[count_i,j,:] = average_vector2\n            elif j == len1+5:\n                X_train3[count_i,j,:] = average_vector3\n            elif j == len1+6:\n                X_train3[count_i,j,:] = final_pos_tags_data[i] \n            elif j == len1+7:\n                X_train3[count_i,j,:] = final_pos_data[i]\n            elif j == len1+8:\n                X_train3[count_i,j,:] = final_tokens_data[i]\n            elif j == len1+9:\n                X_train3[count_i,j,:] = final_dep_data[i]\n            else:\n                X_train3[count_i,j,:] = empty_word\n\n        Y_train3[count_i,:] = dummy_y[i]\n        count_i += 1\n\n\n    count_i = 0\n    for i in testing_indices[givenIndex]:\n        len1 = len(sen[i][0])\n        average_vector1 = np.zeros(512).astype(np.float32)\n        average_vector2 = np.zeros(512).astype(np.float32)\n        average_vector3 = np.zeros(512).astype(np.float32)\n        for j in range(max(maxlength)+10):\n            if j < len1:\n                X_test3[count_i,j,:] = doc2vec_model[sen[i][0][j]]\n                average_vector1 += result_train1[i, tfidf1.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n                average_vector2 += result_train2[i, tfidf2.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]  \n                average_vector3 += result_train3[i, tfidf3.vocabulary_[sen[i][0][j]]] * doc2vec_model[sen[i][0][j]]\n            #elif j >= len1 and j < len1 + 379:\n            #    X_test3[count_i,j,:] = glove_data[i, j-len1, :]\n            elif j == len1:\n                X_test3[count_i,j,:] = tfidf_data1[i]\n            elif j == len1 + 1:\n                X_test3[count_i,j,:] = tfidf_data2[i]\n            elif j == len1+2:\n                X_test3[count_i,j,:] = tfidf_data3[i]\n            elif j == len1+3:\n                X_test3[count_i,j,:] = average_vector1\n            elif j == len1+4:\n                X_test3[count_i,j,:] = average_vector2\n            elif j == len1+5:\n                X_test3[count_i,j,:] = average_vector3\n            elif j == len1+6:\n                X_test3[count_i,j,:] = final_pos_tags_data[i]\n            elif j == len1+7:\n                X_test3[count_i,j,:] = final_pos_data[i]\n            elif j == len1+8:\n                X_test3[count_i,j,:] = final_tokens_data[i]\n            elif j == len1+9:\n                X_test3[count_i,j,:] = final_dep_data[i]\n            else:\n                X_test3[count_i,j,:] = empty_word\n\n        Y_test3[count_i,:] = dummy_y[i]\n        count_i += 1\n        \n    return X_train3, X_test3, Y_train3, Y_test3\n    ","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"1c0d8684-466d-48e6-9afa-bc30ff388a26","_uuid":"b7d99866c070dbf1851fa20d0a242cf1623997ee","collapsed":true,"scrolled":true,"trusted":true},"cell_type":"code","source":"model = Sequential()\nmodel.add(Conv1D(filters=128, kernel_size=9, padding='same', activation='relu', input_shape=(max(maxlength)+10,512)))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(0.25))\nmodel.add(Conv1D(filters=128, kernel_size=7, padding='same', activation='relu'))\nmodel.add(Dropout(0.25))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(Dropout(0.25))\nmodel.add(Conv1D(filters=128, kernel_size=5, padding='same', activation='relu'))\nmodel.add(Dropout(0.25))\n#model.add(MaxPooling1D(pool_size=2))\n#model.add(Dropout(0.25))\n#model.add(Conv1D(filters=32, kernel_size=2, padding='same', activation='relu'))\n#model.add(Dropout(0.25))\n#model.add(MaxPooling1D(pool_size=2))\n#model.add(Dropout(0.25))\n\n#model.add(Bidirectional(LSTM(50, dropout=0.3, recurrent_dropout=0.2, return_sequences=True)))\nmodel.add(Bidirectional(LSTM(50, dropout=0.25, recurrent_dropout=0.2)))\nmodel.add(Dense(4, activation='softmax'))\nmodel.compile(loss='binary_crossentropy', optimizer='Adam', metrics=['accuracy'])\nprint(model.summary())","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a8ec219d-90d2-4f03-97ac-52323bd112c8","_uuid":"a81ad9005cf0c09a134930e20e9e14d3809e3c82","collapsed":true,"scrolled":false,"trusted":true},"cell_type":"code","source":"from sklearn.metrics import accuracy_score\nfrom keras.callbacks import ModelCheckpoint \n\nfinal_accuracies = []\n    \nfilename = 'weights.best.from_scratch%s.hdf5' % 9\ncheckpointer = ModelCheckpoint(filepath=filename, verbose=1, save_best_only=True)\nX_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(9)\nmodel.fit(X_train3, Y_train3, epochs=10, batch_size=512, callbacks=[checkpointer], validation_data=(X_test3, Y_test3))\nmodel.load_weights(filename)\n\nfor i in range(10):\n    filename = 'weights.best.from_scratch%s.hdf5' % i\n    checkpointer = ModelCheckpoint(filepath=filename, verbose=1, save_best_only=True)\n    X_train3, X_test3, Y_train3, Y_test3 = extractTrainingAndTestingData(i)\n    model.fit(X_train3, Y_train3, epochs=10, batch_size=512, callbacks=[checkpointer], validation_data=(X_test3, Y_test3))\n    model.load_weights(filename)\n    predicted = np.rint(model.predict(X_test3))\n    final_accuracies.append(accuracy_score(Y_test3, predicted))\n    print(accuracy_score(Y_test3, predicted))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d7883c70-f74b-4389-85fe-ff575dd15a61","_uuid":"2d96181354f0499ae115b2041997108afa8946dd","collapsed":true,"trusted":true},"cell_type":"code","source":"print(sum(final_accuracies) / len(final_accuracies))","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"a81ce0af-44a8-4081-8a54-73c8ffc97bdf","_uuid":"e9c5ad94f4a4c5ad58e9e588782fc544839b8169","collapsed":true,"trusted":true},"cell_type":"code","source":"","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.4","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}